
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>🔥 이제는 QWEN으로 편집한다! 캐릭터 일관성 & (Inpaint + ControlNet 완벽 활용) 4가지 워크플로우</title>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            line-height: 1.7;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }
        h1 {
            font-size: 2.2em;
            color: #2c3e50;
            margin-bottom: 25px;
            text-align: center;
        }
        h2 {
            font-size: 1.8em;
            color: #00796b;
            border-bottom: 2px solid #b2ebf2;
            padding-bottom: 10px;
            margin-top: 40px;
            margin-bottom: 20px;
        }
        h3 {
            font-size: 1.4em;
            color: #004d40;
            margin-top: 30px;
            margin-bottom: 15px;
            border-left: 4px solid #4dd0e1;
            padding-left: 10px;
        }
        p {
            margin-bottom: 15px;
        }
        ul {
            list-style-type: disc;
            margin: 0 0 15px 25px;
            padding: 0;
        }
        li {
            margin-bottom: 8px;
        }
        strong {
            font-weight: 700;
            color: #004d40;
        }
        .video-meta {
            margin-bottom: 25px;
            padding: 15px;
            background-color: #f8f9fa;
            border-left: 5px solid #00796b;
            border-radius: 4px;
        }
        .video-meta p {
            margin: 5px 0;
            font-size: 15px;
        }
        .video-meta a {
            color: #00796b;
            text-decoration: none;
            font-weight: bold;
        }
        .video-meta a:hover {
            text-decoration: underline;
        }
        .overview-box {
            background-color: #e0f7fa;
            border-left: 5px solid #00796b;
            padding: 20px;
            margin-bottom: 30px;
            border-radius: 8px;
            font-style: italic;
            font-size: 1.1em;
        }
        .definition {
            font-size: 0.9em;
            color: #555;
            margin-top: -10px;
            margin-bottom: 15px;
            padding-left: 15px;
            border-left: 3px solid #b2ebf2;
            background-color: #f7fcff;
            padding: 8px 12px;
            border-radius: 4px;
        }
        .definition a {
            color: #00796b;
            font-weight: bold;
            text-decoration: none;
        }
        .definition a:hover {
            text-decoration: underline;
        }
        .summary-section {
            background-color: #f0f8ff;
            padding: 25px;
            border-radius: 8px;
            border: 1px solid #d0e0ff;
            margin-top: 40px;
        }
        .summary-section h2 {
            color: #2a6fdb;
            border-bottom-color: #a7d9f7;
        }
        .note-info {
            background-color: #e6f7ff;
            border-left: 4px solid #3399ff;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }
        .timestamp {
            font-size: 0.9em;
            color: #777;
            margin-left: 10px;
        }
    </style>
</head>
<body>
    <h1>🔥 이제는 QWEN으로 편집한다! 캐릭터 일관성 & (Inpaint + ControlNet 완벽 활용) 4가지 워크플로우</h1>

    <div class="video-meta">
        <p><strong>videoId:</strong> NP91sTGdRpU</p>
        <p><strong>링크:</strong> <a href="https://www.youtube.com/watch?v=NP91sTGdRpU" target="_blank">https://www.youtube.com/watch?v=NP91sTGdRpU</a></p>
        <p><strong>제목:</strong> 🔥 이제는 QWEN으로 편집한다! 캐릭터 일관성 & (Inpaint + ControlNet 완벽 활용) 4가지 워크플로우</p>
        <p><strong>영상 길이:</strong> 00시간34분58초(2098초)</p>
    </div>

    <div class="overview-box">
        <p>이 영상은 ComfyUI에서 새로 출시된 Qwen Image Edit 모델을 활용하여 이미지를 편집하는 다양한 방법을 소개합니다. ComfyUI의 최신 기능 설명부터, 마스크를 사용하지 않는 직접 편집, 마스크를 활용한 정교한 인페인트, ControlNet을 이용한 포즈 전송, 그리고 여러 이미지를 합쳐 디테일을 추가하는 고급 워크플로우까지 총 4가지 편집 방식을 단계별로 설명하며 Qwen Image Edit의 강력한 기능을 시연합니다.</p>
    </div>

    <h2>내용</h2>
    <h3>[ComfyUI 업데이트 및 새로운 기능] <span class="timestamp">(00:00:00)</span></h3>
    <ul>
        <li><strong>ComfyUI 최신 버전 업데이트</strong> <span class="timestamp">(00:00:06)</span>: ComfyUI를 최신 버전(0.3.52)으로 업데이트하는 과정과 에러 발생 시 버전 맞추는 방법을 언급합니다.</li>
        <li><strong>새로운 UI 기능 소개</strong> <span class="timestamp">(00:00:51)</span>:
            <ul>
                <li><strong>작업 영역 미리 보기 창</strong>: 현재 작업 중인 부분을 작은 창으로 확인하고, 워크플로우가 길어질 경우 해당 위치로 빠르게 이동할 수 있습니다.</li>
                <li><strong>전체 보기 버튼</strong>: 화면에 있는 모든 노드를 한눈에 볼 수 있도록 전체 보기 기능을 제공합니다.</li>
                <li><strong>서브 그래프 기능</strong> <span class="timestamp">(00:01:23)</span>: 여러 노드를 선택하여 하나의 서브 노드로 그룹화하여 워크플로우의 복잡도를 줄일 수 있습니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=서브%20그래프" target="_blank">서브 그래프 (Sub-graph)</a>: 여러 개의 개별 노드를 논리적으로 묶어 하나의 간소화된 노드처럼 다루는 기능입니다. 복잡한 워크플로우를 정리하고 가독성을 높이는 데 유용합니다.</p>
                </li>
            </ul>
        </li>
    </ul>

    <h3>[Qwen Image Edit 모델 소개 및 설정] <span class="timestamp">(00:02:43)</span></h3>
    <ul>
        <li><strong>Qwen Image Edit 모델 개요</strong> <span class="timestamp">(00:02:43)</span>: Qwen Image Edit는 기존의 Qwen Image 모델에서 이미지를 편집하는 기능이 추가된 모델이며, Flux Context와 유사하게 작동합니다. Qwen Image Union Control을 통해 ControlNet도 사용 가능합니다.</li>
        <li><strong>기본 모델 구성 요소</strong> <span class="timestamp">(00:03:09)</span>:
            <ul>
                <li><strong>모델, 클립, VAE</strong>: 기존 Qwen Image와 동일한 클립 및 VAE를 사용하지만, 메인 모델은 Qwen Image Edit로 변경됩니다.</li>
                <li><strong>로라 (LoRa)</strong> <span class="timestamp">(00:03:30)</span>: Qwen Image Lightning 4-step LoRa를 사용합니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=LoRa" target="_blank">LoRa (Low-Rank Adaptation)</a>: 대규모 사전 학습 모델의 파라미터를 효율적으로 미세 조정하는 기술로, 적은 계산 자원으로 특정 스타일이나 캐릭터를 학습시킬 때 사용됩니다.</p>
                </li>
                <li><strong>샘플링 최적화</strong> <span class="timestamp">(00:03:35)</span>: Model Sampling Auraflow 및 CFG Nom 서브 노드를 연결하여 샘플링 성능과 출력 품질을 향상시킵니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=CFG%20Nom" target="_blank">CFG Nom (Classifier-Free Guidance Normalization)</a>: 이미지 생성 시 텍스트 프롬프트의 영향력을 조절하는 CFG(Classifier-Free Guidance) 값을 정규화하여 출력 이미지의 품질과 다양성을 개선하는 기법입니다.</p>
                </li>
                <li><strong>새로운 노드</strong> <span class="timestamp">(00:03:50)</span>: 'Text Encode Qwen Image Edit'는 기존 클립 텍스트 인코더에 이미지 정보를 함께 포함시켜 컨디셔닝(Conditioning)으로 전달하여 이미지를 편집할 수 있게 합니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=컨디셔닝" target="_blank">컨디셔닝 (Conditioning)</a>: AI 모델이 이미지를 생성하거나 편집할 때 특정 조건(텍스트 프롬프트, 이미지 등)을 부여하여 결과물에 영향을 미치는 과정입니다.</p>
                </li>
            </ul>
        </li>
        <li><strong>권장 모델 설정 값</strong> <span class="timestamp">(00:04:24)</span>:
            <ul>
                <li>Qwen Image Edit 오피셜 베이스 모델: 40 스텝, CFG 4.</li>
                <li>FP8 버전: 20 스텝, CFG 2.5.</li>
                <li>LoRa 사용 시: 4 스텝, CFG 1 (본 영상에서 사용).
                    <p class="definition">※ <a href="https://oo.ai/search?q=FP8" target="_blank">FP8</a>: 8비트 부동 소수점(Floating Point) 형식으로, 모델 크기를 줄이고 계산 속도를 높여 VRAM 사용량을 최적화하는 데 사용됩니다.</p>
                </li>
            </ul>
        </li>
    </ul>

    <h3>[Qwen Image Edit 모델 다운로드 및 설치] <span class="timestamp">(00:04:50)</span></h3>
    <ul>
        <li><strong>GGUF 모델 다운로드</strong> <span class="timestamp">(00:04:50)</span>: Hugging Face에서 'Qwen Image Edit GGUF' 모델(Q4_K_M 버전 권장)을 다운로드합니다. VRAM 용량에 따라 Q8 (넉넉), Q3 (부족) 버전을 선택할 수 있습니다.
            <p class="definition">※ <a href="https://oo.ai/search?q=GGUF" target="_blank">GGUF</a>: GPT-GEneration Unified Format의 약자로, 대규모 언어 모델(LLM)과 확산 모델(Diffusion Model)을 효율적으로 저장하고 로드하기 위한 파일 형식입니다. 특히 CPU나 저사양 GPU에서도 실행할 수 있도록 최적화되어 있습니다.</p>
            <p class="definition">※ <a href="https://oo.ai/search?q=VRAM" target="_blank">VRAM (Video Random Access Memory)</a>: 그래픽 카드에 탑재된 메모리로, 이미지 및 비디오 처리에 필요한 데이터를 저장합니다. VRAM 용량이 클수록 고해상도 이미지 처리나 복잡한 모델 실행에 유리합니다.</p>
        </li>
        <li><strong>모델 파일 배치</strong> <span class="timestamp">(00:05:07)</span>:
            <ul>
                <li><strong>Qwen Image Text Encoder</strong>: <code>ComfyUI/models/text_encoders</code> 폴더.</li>
                <li><strong>VAE</strong>: <code>ComfyUI/models/vae</code> 폴더.</li>
                <li><strong>LoRa</strong>: <code>ComfyUI/models/loras</code> 폴더.</li>
                <li><strong>Qwen Edit GGUF 모델</strong>: <code>ComfyUI/models/unef</code> 폴더 또는 <code>ComfyUI/data/models/diffusion_models</code> (Stability Matrix 사용자).</li>
            </ul>
        </li>
        <li><strong>ComfyUI GGUF 커스텀 노드 설치</strong> <span class="timestamp">(00:07:50)</span>: GGUF 모델을 불러오기 위해 ComfyUI Manager에서 'ComfyUI-GGUF' 커스텀 노드를 설치합니다. KJNodes 및 RG3ComfyuiUseEverywhere와 같은 다른 커스텀 노드들도 필요할 수 있습니다.
            <p class="definition">※ <a href="https://oo.ai/search?q=커스텀%20노드" target="_blank">커스텀 노드 (Custom Node)</a>: ComfyUI의 기본 노드 기능 외에 사용자가 직접 개발하거나 타인이 개발한 추가 기능을 워크플로우에 통합하여 사용할 수 있게 해주는 노드입니다. 특정 작업을 자동화하거나 새로운 모델을 지원하는 데 활용됩니다.</p>
        </li>
    </ul>

    <h3>[워크플로우 1: 마스크 없는 직접 이미지 편집] <span class="timestamp">(00:09:40)</span></h3>
    <ul>
        <li><strong>워크플로우 설정</strong> <span class="timestamp">(00:09:40)</span>:
            <ul>
                <li>UNEF Loader를 통해 GGUF 모델을 불러옵니다.</li>
                <li>SageAttention 노드 (선택 사항)를 연결합니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=SageAttention" target="_blank">SageAttention</a>: 특정 커스텀 노드로, 모델의 어텐션 메커니즘을 조절하여 이미지 생성 품질을 개선하는 데 사용됩니다.</p>
                </li>
                <li>모델 샘플링 Auraflow 및 CFGNom 노드를 연결합니다.</li>
                <li>LoRa 로더 (Anything Everywhere 노드)를 통해 'Qwen Image Lightning 4step V1' LoRa를 불러옵니다.</li>
                <li>VAE 및 Clip 모델을 연결합니다.</li>
                <li>원하는 이미지를 로드하고, 'Text Encode Qwen Image Edit' 노드에 연결하여 컨디셔닝 정보를 포함시킵니다.</li>
                <li>KSampler를 통해 이미지를 생성합니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=KSampler" target="_blank">KSampler</a>: ComfyUI의 핵심 노드 중 하나로, 확산 모델(Diffusion Model)에서 이미지 생성을 실제로 수행하는 역할을 합니다. Latent Space에서 노이즈를 제거하며 최종 이미지를 만들어냅니다.</p>
                </li>
            </ul>
        </li>
        <li><strong>예시 1: 머리색 변경</strong> <span class="timestamp">(00:10:39)</span>: '빨간 머리로 변경한다'는 프롬프트를 입력하면, 전체 이미지의 톤과 스타일이 약간 변경되면서 머리색이 바뀝니다. 원하는 부분만 변경되지 않고 전체적인 변화가 발생하는 것을 확인합니다.</li>
        <li><strong>Qwen Image Edit의 다양한 활용 예시 (공식 자료 기반)</strong> <span class="timestamp">(00:11:22)</span>:
            <ul>
                <li><strong>캐릭터 일관성 유지</strong>: 레퍼런스 캐릭터의 일관성을 유지하며 다양한 포즈와 상황의 이미지를 생성 (MBTI 예시).</li>
                <li><strong>회전 및 시점 변경</strong>: 프롬프트를 통해 캐릭터의 옆모습을 앞모습으로, 혹은 좌우/뒷모습으로 변경.</li>
                <li><strong>이미지 스타일 변경</strong>: 지브리 스타일, 3D 카툰 스타일 등으로 변환.</li>
                <li><strong>오브젝트 추가/제거</strong>: 특정 부분에 오브젝트를 추가하거나, 머리카락 같은 부분을 제거.</li>
                <li><strong>색상 및 배경 변경</strong>: 특정 글자의 색상을 변경하거나, 배경을 변경, 옷을 변경.</li>
            </ul>
        </li>
        <li><strong>예시 2: 호랑이 캐릭터 변환</strong> <span class="timestamp">(00:12:51)</span>: 원본 호랑이 캐릭터 이미지를 '그림 그리는 모습', '어쿠스틱 기타 치는 모습', '턱시도를 입고 마술 지팡이를 든 마술사' 등으로 변환하며 캐릭터 일관성을 유지하는 것을 시연합니다.</li>
        <li><strong>결론</strong>: 마스크 없이 직접 편집 시 캐릭터 일관성은 잘 유지되지만, 전체 이미지에 변화가 발생할 수 있습니다. <span class="timestamp">(00:15:43)</span></li>
    </ul>

    <h3>[워크플로우 2: 마스크를 이용한 이미지 편집 (인페인트)] <span class="timestamp">(00:16:09)</span></h3>
    <ul>
        <li><strong>문제점 인식</strong> <span class="timestamp">(00:16:09)</span>: 마스크 없는 편집 방식의 한계 (예: 오른쪽 여성의 머리색만 바꾸려 했으나 가운데 여성의 머리색까지 변경되고 전체 톤 변화 발생).</li>
        <li><strong>해결책</strong> <span class="timestamp">(00:17:21)</span>: 마스크를 활용한 정교한 인페인트(Inpaint) 기능 도입.
            <ul>
                <li><strong>필수 커스텀 노드</strong>: 'Inpaint Crop & Stitch' 노드를 설치합니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=Inpaint%20Crop%20&%20Stitch" target="_blank">Inpaint Crop & Stitch</a>: 이미지를 특정 마스크 영역만큼 잘라내어(Crop) 해당 영역을 편집한 후, 다시 원본 이미지에 매끄럽게 붙여넣는(Stitch) 기능을 제공하는 커스텀 노드입니다. 정교한 부분 편집에 유용합니다.</p>
                </li>
            </ul>
        </li>
        <li><strong>워크플로우 설정</strong> <span class="timestamp">(00:18:00)</span>:
            <ul>
                <li>원본 이미지에서 편집할 영역을 마스크로 칠합니다.</li>
                <li>'Inpaint Crop & Stitch' 노드를 사용하여 마스크 영역만 잘라내고, 해당 부분의 이미지와 마스크를 생성합니다.</li>
                <li>잘라낸 이미지를 리사이즈한 후, 'Text Encode Qwen Image Edit' 노드에 연결하여 컨디셔닝 정보를 전달합니다.</li>
                <li>'Inpaint Model Conditioning' 노드, KSampler, VAE Decoude 등을 연결합니다.</li>
                <li>'Differential Diffusion Advanced' 노드 (KJNodes)를 사용하여 모델과 레이턴트 이미지를 연결합니다.</li>
                <li>마지막으로 'Inpaint Stitch' 노드를 통해 새로 생성된 마스크 영역 이미지를 원본 이미지에 다시 붙여 넣습니다.</li>
            </ul>
        </li>
        <li><strong>예시: 오른쪽 여성의 머리색만 빨간색으로 변경</strong> <span class="timestamp">(00:20:00)</span>: 마스크를 칠한 부분(머리카락)만 정확히 빨간색으로 변경되고, 다른 부분은 전혀 변경되지 않는 것을 확인합니다.</li>
        <li><strong>예시: 티셔츠 색깔만 파란색으로 변경</strong> <span class="timestamp">(00:21:20)</span>: 마스크를 칠한 티셔츠 부분만 파란색으로 변경되어, 원하는 부분만 정확히 편집이 가능함을 보여줍니다.</li>
        <li><strong>결론</strong>: 마스크를 사용하면 원하는 부분만 정교하게 편집하여 이미지의 다른 부분이 변형되는 것을 방지할 수 있습니다. <span class="timestamp">(00:22:32)</span></li>
    </ul>

    <h3>[워크플로우 3: ControlNet을 이용한 포즈 전송] <span class="timestamp">(00:22:42)</span></h3>
    <ul>
        <li><strong>ControlNet 활용</strong> <span class="timestamp">(00:22:42)</span>: Qwen Image Edit에 ControlNet 기능을 통합하여 포즈 전송 등의 작업을 수행합니다.</li>
        <li><strong>Qwen Image ControlNet 모델</strong> <span class="timestamp">(00:23:13)</span>: Dex, Canny, Inpaint 모델이 있습니다. (본 영상에서는 Dex 사용)</li>
        <li><strong>ControlNet 모델 다운로드 및 설치</strong> <span class="timestamp">(00:23:36)</span>: 'ComfyUI-aux'에서 제공하는 ControlNet 모델(Qwen Image Canny, Dex, Inpaint)을 다운로드하여 <code>ComfyUI/models/model_patches</code> 폴더에 배치합니다.</li>
        <li><strong>필수 커스텀 노드</strong> <span class="timestamp">(00:24:29)</span>: 'Dex Anything V2' (ComfyUI ControlNet Aux 커스텀 노드)를 설치합니다.
            <p class="definition">※ <a href="https://oo.ai/search?q=ComfyUI%20ControlNet%20Aux" target="_blank">ComfyUI ControlNet Aux</a>: ControlNet 모델을 ComfyUI에서 쉽게 활용할 수 있도록 다양한 프리프로세서와 ControlNet 로더 노드를 제공하는 커스텀 노드입니다. Canny, Dex, OpenPose 등 다양한 ControlNet 기능을 지원합니다.</p>
            <p class="definition">※ <a href="https://oo.ai/search?q=Dex%20(ControlNet)" target="_blank">Dex (ControlNet)</a>: ControlNet에서 이미지의 깊이(Depth) 정보를 추출하여 이를 기반으로 새로운 이미지를 생성하는 데 사용되는 모델입니다. 원본 이미지의 3D 구조나 포즈를 유지하면서 새로운 스타일이나 내용의 이미지를 만들 때 유용합니다.</p>
        </li>
        <li><strong>워크플로우 설정</strong> <span class="timestamp">(00:25:35)</span>:
            <ul>
                <li>원본 이미지를 로드하고, 'Scale Image to Total Pixels' 노드를 통해 이미지 크기를 ComfyUI 처리하기에 적합한 사이즈로 조절합니다.</li>
                <li>'Dex Anything V2' 노드에 이미지를 연결하여 Dex 이미지를 생성합니다.</li>
                <li>'Text Encode Qwen Image Edit' 노드에서는 이미지 연결을 해제하고 프롬프트만 전달합니다 (Dex 이미지가 포즈 참조). <span class="timestamp">(00:27:26)</span></li>
                <li>ControlNet 모델(Dex)을 연결하고, KSampler를 통해 이미지를 생성합니다.</li>
            </ul>
        </li>
        <li><strong>예시: 포즈 전송</strong> <span class="timestamp">(00:27:56)</span>: 원본 이미지의 포즈를 참조하여 '19세 한국 소녀가 아이폰을 들고 바닷가에서 사진을 찍고 있다'는 프롬프트로 새로운 이미지를 생성합니다.</li>
        <li><strong>결론</strong>: Dex ControlNet 모델을 활용하면 원본 이미지의 포즈를 정확히 따라가면서 새로운 캐릭터와 배경의 이미지를 생성할 수 있습니다. Dex가 Canny보다 포즈를 더 잘 따라한다고 언급합니다. <span class="timestamp">(00:28:48)</span></li>
    </ul>

    <h3>[워크플로우 4: 여러 이미지 합치기 및 디테일 추가] <span class="timestamp">(00:29:03)</span></h3>
    <ul>
        <li><strong>여러 이미지 합치기</strong> <span class="timestamp">(00:29:03)</span>: 'Image Stitch' 노드를 사용하여 두 개 이상의 이미지를 하나로 합쳐서 Qwen Image Edit에 입력합니다.</li>
        <li><strong>워크플로우 설정</strong> <span class="timestamp">(00:29:44)</span>:
            <ul>
                <li>두 개의 이미지를 로드하고, 'Image Stitch' 노드로 합칩니다.</li>
                <li>합쳐진 이미지를 'Text Encode Qwen Image Edit' 노드에 연결하여 컨디셔닝 정보를 전달합니다.</li>
                <li>'MT SD Latent Image' 노드를 사용하여 레이턴트 이미지를 생성하고 KSampler에 연결합니다. <span class="timestamp">(00:30:35)</span>
                    <p class="definition">※ <a href="https://oo.ai/search?q=MT%20SD%20Latent%20Image" target="_blank">MT SD Latent Image</a>: ComfyUI의 Latent 공간에서 직접 Latent 이미지를 생성하는 노드입니다. 특정 해상도와 배치 사이즈로 이미지 생성을 시작할 때 사용됩니다.</p>
                </li>
            </ul>
        </li>
        <li><strong>예시: 두 캐릭터가 포옹하는 장면 생성</strong> <span class="timestamp">(00:31:36)</span>: 합쳐진 두 캐릭터 이미지를 기반으로 '두 캐릭터가 포옹한다'는 프롬프트로 이미지를 생성합니다. (배경 프롬프트 추가 시 더 좋은 결과 기대)</li>
        <li><strong>Flux Crea (Nunchaku)를 이용한 디테일 추가</strong> <span class="timestamp">(00:31:48)</span>:
            <ul>
                <li>생성된 이미지의 디테일을 높이기 위해 이전 영상에서 다뤘던 'Flux Crea' (Nunchaku Context) 워크플로우를 사용합니다.</li>
                <li>디노이즈(Denoise) 강도는 0.3 정도로 설정하며, 0.2~0.5 범위에서 최적 값을 찾습니다.
                    <p class="definition">※ <a href="https://oo.ai/search?q=Flux%20Crea%20(Nunchaku)" target="_blank">Flux Crea (Nunchaku)</a>: 이미지에 추가적인 디테일과 사실감을 부여하기 위해 사용되는 특정 워크플로우 또는 모델 구성입니다. 특히 Stable Diffusion과 같은 확산 모델에서 이미지의 미세한 부분을 개선하는 데 활용됩니다.</p>
                    <p class="definition">※ <a href="https://oo.ai/search?q=디노이즈%20강도" target="_blank">디노이즈 강도 (Denoise Strength)</a>: 이미지 생성 과정에서 원본 이미지의 노이즈를 얼마나 많이 제거하고 새로운 이미지로 변형시킬 것인지를 결정하는 파라미터입니다. 값이 높을수록 원본 이미지에서 크게 변형되며, 낮을수록 원본의 특성을 유지합니다.</p>
                </li>
            </ul>
        </li>
        <li><strong>결론</strong>: 여러 이미지를 합쳐 기본적인 장면을 생성하고, Flux Crea로 디테일을 추가하여 더욱 자연스럽고 고품질의 결과물을 얻을 수 있습니다. <span class="timestamp">(00:32:50)</span></li>
    </ul>

    <div class="summary-section">
        <h2>요약</h2>
        <p>이 영상은 ComfyUI의 최신 Qwen Image Edit 모델을 활용한 4가지 이미지 편집 워크플로우를 상세히 다룹니다. 먼저 ComfyUI의 업데이트된 UI 기능(미리 보기, 서브 그래프)을 소개하며 작업 효율성을 강조합니다. 이어 Qwen Image Edit 모델의 구성 요소와 설정(LoRa, Auraflow, CFG Nom, 새로운 Text Encode 노드) 및 GGUF 모델 다운로드 방법을 설명합니다. 첫 번째 워크플로우에서는 마스크 없이 직접 프롬프트로 이미지를 편집하는 방법을 시연하지만, 원치 않는 부분까지 변경될 수 있는 한계를 지적합니다. 두 번째 워크플로우에서는 'Inpaint Crop & Stitch' 커스텀 노드를 사용하여 마스크 영역만 정교하게 편집함으로써 특정 부분만 변경하는 방법을 보여줍니다. 세 번째 워크플로우에서는 ControlNet(Dex 모델)을 Qwen Image Edit와 통합하여 원본 이미지의 포즈를 유지한 채 새로운 캐릭터와 배경의 이미지를 생성하는 방법을 소개합니다. 마지막으로, 여러 이미지를 합쳐 기본적인 장면을 생성한 후 'Flux Crea (Nunchaku)' 워크플로우로 디테일을 추가하여 이미지 품질을 높이는 방법을 시연하며, Qwen Image Edit의 강력하고 다재다능한 편집 능력을 강조합니다.</p>
    </div>

</body>
</html>